{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Enhancing Data Handling Efficiency and Memory Management\n",
    "\n",
    "In our endeavor to enhance data handling efficiency and optimize memory usage, we have devised a methodical approach. Given the considerable size of our dataset—roughly 300,000 columns and 535 rows—processing poses a formidable challenge. Hence, we adopt a meticulous strategy: the raw file is imported with efficiency in mind, then meticulously segmented into multiple files. Each file contains 10,000 columns, systematically labeled as data_0, data_1, ..., data_29. Furthermore, we conduct a curation process, removing superfluous columns to streamline data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing CPU Resources with Spark Session Initialization\n",
    "First, we'll import the necessary libraries. Next, we'll initiate a Spark session and configure it to harness the maximum CPU resources available on the PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8072bc9c-8e81-4164-a1b6-eeb1b7674ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, ChiSqSelector\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import gc, psutil\n",
    "\n",
    "# Create a Spark session\n",
    "spark = (SparkSession.builder\n",
    "        .appName(\"DementiaData\")\n",
    "        .config(\"spark.driver.memory\", \"7g\")\n",
    "        .config(\"spark.driver.cores\", \"3\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Script for Dementia Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll gather all the necessary files for processing into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location\n",
    "data_read_locations = [\"hdfs://localhost:9000/user/hdfs/dimentia_model/JanBDRcount.raw\", # Location of data where data is raw\n",
    "                       \"Distributed/\", # Location of data where batch size is 100\n",
    "                       \"combined.csv\" # Location of combined data after feature selection\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preprocessing, we reading data in text format. This script efficiently manages data reading, column name adjustments, and data structuring for analysis. Key steps include setting up the file location, reading the file, adjusting column names by replacing colons with underscores, and using Spark's `read.text` method to parse the file and split values into columns with `withColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's start with file reading process.\n",
      "Completed with reading the file.\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's start with file reading process.\")\n",
    "\n",
    "# We are reading the first column using python\n",
    "with open(data_read_locations[0], \"r\") as file:\n",
    "    column_names = file.readline().strip().split(\" \")\n",
    "\n",
    "# Changing the column names to avoid the issues\n",
    "column_names = [col_name.replace(':', '_') for col_name in column_names]\n",
    "\n",
    "# Read the text file as a single column\n",
    "read_raw_file = spark.read.text(data_read_locations[0])\n",
    "\n",
    "# Split each line into individual columns based on space delimiter\n",
    "read_raw_file = read_raw_file.withColumn(\"columns\", split(read_raw_file[\"value\"], \" \").cast(\"array<int>\"))\n",
    "\n",
    "print(\"Completed with reading the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the column names with the actual column names exported earlier using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f72805a-0fe2-498a-a4b8-f51dc0480a13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's create the dataframe and change the column names.\")\n",
    "dementia_data = (read_raw_file.selectExpr([f\"columns[{i}] as {col_name}\" \n",
    "                                               for i, col_name in enumerate(column_names)\n",
    "                                               if col_name not in \"FID IID PAT MAT\".split(\" \")]\n",
    "                                         )\n",
    "                )\n",
    "print(\"Completed with dataframe creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlined Batch Processing of Dementia Dataset Columns\n",
    "Upon acquiring the column names from the dementia dataset, we segment them into batches, each comprising 100 columns. These batches are pivotal in constructing separate dataframes, with dimensions of 100 columns and 535 rows, across a total of 2977 files. This approach not only enhances data organization but also expedites processing by enabling parallel execution across multiple files. Additionally, the provision of progress updates for each batch ensures transparency and facilitates real-time monitoring, thereby optimizing the chunk creation process and overall data management efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d14428-d53a-436e-81d8-c83137e1a924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's create the batch size and create diffrent files with columns of defined batch size.\")\n",
    "\n",
    "column_names = dementia_data.columns\n",
    "# Create a batch size of 100 columns with each file\n",
    "batch_size = 100\n",
    "\n",
    "# Copying column names in chunks of batch size\n",
    "column_names_batches = [column_names[i:i+batch_size] for i in range(0, len(column_names), batch_size)]\n",
    "\n",
    "# Creating dataframe for each 100 columns and exporting as csv\n",
    "for index,batch in enumerate(column_names_batches):\n",
    "    dementia_data.select(*batch).coalesce(1).write.csv(f\"{data_read_locations[1]}/Data__{index}\",header=True,mode=\"overwrite\")\n",
    "    print(f\"Completed with {index} chunk\")\n",
    "\n",
    "print(\"Completed with chunks creation.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "big_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
